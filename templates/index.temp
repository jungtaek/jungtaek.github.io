            <p class="paragraph-120-bold">NEWS</p>

            <ul class="general-list no-bullet-list">

                <li>[May 2024] My work "<a href="https://arxiv.org/abs/2402.07341" target="_blank">Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization</a>" with Prof. <a href="https://kwangsungjun.github.io" target="_blank">Kwang-Sung Jun</a> has been accepted at <a href="https://icml.cc/Conferences/2024" target="_blank">ICML-2024</a>.</li>
                <li>[February 2024] My new paper "<a href="https://arxiv.org/abs/2402.07341" target="_blank">Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization</a>" has been released at <a href="https://arxiv.org" target="_blank">arXiv</a>.</li>
                <li>[January 2024] I will serve as a reviewer for <a href="https://www.auai.org/uai2024/" target="_blank">UAI-2024</a>.</li>
                <li>[January 2024] My paper "<a href="https://arxiv.org/abs/2310.07174" target="_blank">Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions</a>" with Jeongbeen Yoon and Prof. Minsu Cho has been accepted at <a href="https://iclr.cc/Conferences/2024" target="_blank">ICLR-2024</a>.</li>
                <li>[January 2024] My new paper "<a href="https://arxiv.org/abs/2401.01981" target="_blank">Beyond Regrets: Geometric Metrics for Bayesian Optimization</a>" is released at <a href="https://arxiv.org" target="_blank">arXiv</a>.</li>
                <li>[January 2024] Happy new year!</li>
                <li>[December 2023] I will serve as a reviewer for <a href="https://icml.cc/Conferences/2024" target="_blank">ICML-2024</a>.</li>
                <li>[November 2023] My paper "<a href="" target="_blank">Multi-BOWS: Multi-Fidelity Multi-Objective Bayesian Optimization with Warm Starts for Nanophotonic Structure Design</a>" with Mingxuan Li, Yirong Li, Prof. Andrés Gómez, Prof. Oliver Hinder, and Prof. Paul W. Leu has been accepted at <a href="https://www.rsc.org/journals-books-databases/about-journals/digital-discovery" target="_blank">RSC Digital Discovery</a>.</li>
                <li>[October 2023] My papers "<a href="https://arxiv.org/abs/2310.19053" target="_blank">Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations</a>" and "<a href="https://arxiv.org/abs/2310.19464" target="_blank">Generative Neural Fields by Mixtures of Neural Implicit Functions</a>" have been released at <a href="https://arxiv.org" target="_blank">arXiv</a>.</li>
                <li>[October 2023] My collaborative paper with Chaeyun Jang, Hyungi Lee, and Prof. Juho Lee has been accepted at <a href="https://neurips2023-enlsp.github.io" target="_blank">the 3rd NeurIPS Workshop on Efficient Natural Language and Speech Processing</a>.</li>
                <li>[October 2023] My new paper with Hyunsoo Chung has been accepted at <a href="https://sslneurips23.github.io" target="_blank">the NeurIPS 2023 Workshop on Self-Supervised Learning - Theory and Practice</a>.</li>
                <li>[October 2023] My new paper "<a href="https://arxiv.org/abs/2310.07174" target="_blank">Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions</a>" with Jeongbeen Yoon and Prof. Minsu Cho has been released at <a href="https://arxiv.org" target="_blank">arXiv</a>.</li>
                <li>[October 2023] My paper "<a href="https://joss.theoj.org/papers/10.21105/joss.05320" target="_blank">BayesO: A Bayesian Optimization Framework in Python</a>" was published in <a href="https://joss.theoj.org" target="_blank">the Journal of Open Source Software</a>.</li>
                <li>[September 2023] Two papers have been accepted at <a href="https://neurips.cc/Conferences/2023" target="_blank">NeurIPS-2023</a>. One paper with Tackgeun You, Mijeong Kim, and Prof. Bohyung Han will be presented at the main track of NeurIPS-2023 and another paper with Mingxuan Li, Prof. Oliver Hinder, and Prof. Paul W. Leu will be presented at the datasets and benchmarks track of NeurIPS-2023.</li>

            </ul>

            <p class="paragraph-100 show-more"><a href="news.html">Show more</a></p>

            <hr>

            <p class="paragraph-120-bold">ABOUT ME</p>
            <p class="paragraph-100">
                Jungtaek Kim is a postdoctoral associate at the University of Pittsburgh, working as a member of the Center for Materials Data Science for Reliability and Degradation. At the University of Pittsburgh, he works with Prof. Paul W. Leu, Prof. Satish Iyengar, Prof. Lucas Mentch, and Prof. Oliver Hinder. Before that, he was a postdoctoral researcher at POSTECH, in the group of Prof. Minsu Cho. He received B.S. in Mechanical Engineering and Computer Science and Engineering from POSTECH in 2015, and Ph.D. in Computer Science and Engineering from POSTECH in 2022, under the supervision of Prof. Seungjin Choi and Prof. Minsu Cho. He interned at Vector Institute and SigOpt (acquired by Intel), during his Ph.D. program. His internship at Vector Institute was carried out under the supervision of Prof. Graham W. Taylor and he worked with Dr. Michael McCourt at SigOpt. He presented his work as the first author or a co-author, at the top-tier machine learning conferences such as NeurIPS, AISTATS, UAI, ICML, and ICLR, and moreover he served as a program committee member or a reviewer for diverse machine learning conferences such as NeurIPS, ICML, AISTATS, UAI, and ICLR. His main research interests are statistical machine learning, Bayesian optimization, and sequential assembly.
            </p>

            <hr>

            <p class="paragraph-120-bold">CURRENT RESEARCH INTERESTS</p>
            <ul class="general-list">
                <li>Statistical Machine Learning</li>
                <li>Bayesian Optimization</li>
                <li>Hyperparameter Optimization</li>
                <li>Automation of Machine Learning</li>
                <li>Sequential Assembly</li>
            </ul>

            <p class="paragraph-120-bold">WORK-IN-PROGRESS PAPERS</p>
            <ul class="general-list">
                <li>Kwang-Sung Jun and Jungtaek Kim. <a href="https://arxiv.org/abs/2402.07341" target="_blank">Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization</a>.</li>
                <li>Jungtaek Kim. <a href="https://arxiv.org/abs/2401.01981" target="_blank">Beyond Regrets: Geometric Metrics for Bayesian Optimization</a>.</li>
                <li>Jungtaek Kim. <a href="https://arxiv.org/abs/2305.15612" target="_blank">Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning</a>.</li>
                <li>Seokjun Ahn, Jungtaek Kim, Minsu Cho, and Jaesik Park. <a href="https://arxiv.org/abs/2210.01021" target="_blank">Sequential Brick Assembly with Efficient Constraint Satisfaction</a>.</li>
            </ul>

            <p class="paragraph-120-bold">ELSEWHERE</p>
            <ul class="general-list">
                <li>Academic Profile: <a href="https://scholar.google.com/citations?user=KXNUYWgAAAAJ&hl=en" target="_blank">Google Scholar</a>, <a href="https://dblp.org/pid/31/3193-1" target="_blank">DBLP</a>, <a href="https://www.semanticscholar.org/author/2629559" target="_blank">Semantic Scholar</a>, <a href="https://orcid.org/0000-0002-1905-1399" target="_blank">ORCID</a></li>
                <li>Social Network: <a href="https://www.linkedin.com/in/jungtaekkim" target="_blank">LinkedIn</a>, <a href="https://twitter.com/jungtaek_kim" target="_blank">Twitter</a></li>
                <li>Development: <a href="https://www.github.com/jungtaekkim" target="_blank">GitHub</a></li>
            </ul>
