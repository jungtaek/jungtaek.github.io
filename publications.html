<!DOCTYPE html>
<html>
    <head>
        <title>PUBLICATIONS | JUNGTAEK KIM</title>
        <link rel="stylesheet" type="text/css" href="mystyle.css">
    </head>
    <body>
        <div>
            <p class="paragraph-200-bold">Jungtaek Kim's Publications / E-prints / Manuscripts / Technical Reports / Dissertation</p>

            <p class="paragraph-100">(* indicates equal contribution.)</p>

            <ol class="general-list">

                <!--ijcai-ecai-2022-a-->
                <li>
                    Jinhwi Lee*, <strong>Jungtaek Kim</strong>*, Hyunsoo Chung, Jaesik Park, and Minsu Cho (2022),<br>
                    <a href="">
                        "Learning to assemble geometric shapes,"</a><br>
                    in <i>Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (<strong>IJCAI-2022</strong>),</i><br>
                    Vienna, Austria, July 23-29, 2022.<br>
                    Acceptance rate: 681/4535 = 15.0%<br>
                </li>

                <!--iclr-2022-a-->
                <li>
                    Rylee Thompson, Boris Knyazev, Elahe Ghalebi, <strong>Jungtaek Kim</strong>, and Graham W. Taylor (2022),<br>
                    <a href="papers/iclr_2022.pdf">
                        "On evaluation metrics for graph generative models,"</a><br>
                    in <i>Proceedings of the Tenth International Conference on Learning Representations (<strong>ICLR-2022</strong>),</i><br>
                    Virtual, April 25-29, 2022.<br>
                    Acceptance rate: 1095/3391 = 32.3%<br>
                </li>

                <!--aistats-2022-a-->
                <li>
                    <strong>Jungtaek Kim</strong> and Seungjin Choi (2022),<br>
                    <a href="papers/aistats_2022.pdf">
                        "On uncertainty estimation by tree-based surrogate models in sequential model-based optimization,"</a><br>
                    in <i>Proceedings of the Twenty-Fifth International Conference on Artificial Intelligence and Statistics (<strong>AISTATS-2022</strong>),</i><br>
                    Virtual, March 28-30, 2022.<br>
                    Acceptance rate: 492/1685 = 29.2%<br>
                </li>

                <!--thesis-2022-->
                <li>
                    <strong>Jungtaek Kim</strong> (2022),<br>
                    <a href="">
                        "Efficient Bayesian optimization: Algorithms, approximation, and regret analysis,"</a><br>
                    <i>Doctoral Dissertation,</i><br>
                    2022.<br>
                </li>

                <!--neurips-2021-a-->
                <li>
                    Hyunsoo Chung*, <strong>Jungtaek Kim</strong>*, Boris Knyazev, Jinhwi Lee, Graham W. Taylor, Jaesik Park, and Minsu Cho (2021),<br>
                    <a href="papers/neurips_2021.pdf">
                        "Brick-by-Brick: Combinatorial construction with deep reinforcement learning,"</a><br>
                    in <i>Advances in Neural Information Processing Systems 34 (<strong>NeurIPS-2021</strong>),</i><br>
                    Virtual, December 6-14, 2021.<br>
                    Acceptance rate: 2344/9122 = 25.7%<br>
                </li>

                <!--ml-2021-a-->
                <li>
                    <strong>Jungtaek Kim</strong>, Michael McCourt, Tackgeun You, Saehoon Kim, and Seungjin Choi (2021),<br>
                    <a href="https://arxiv.org/pdf/1905.09780.pdf">
                        "Bayesian optimization with approximate set kernels,"</a><br>
                    <i>Machine Learning,</i><br>
                    vol. 110, no. 5, pp. 857-879, 2021.<br>
                    Part of Special Issue of the ECML-PKDD-2021 Journal Track;
                    Acceptance rate: 20/107 = 18.7%<br>
                </li>

                <!--ml4eng-2020-a-->
                <li>
                    <strong>Jungtaek Kim</strong>, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, and Jaesik Park (2020),<br>
                    <a href="papers/ml4eng_2020.pdf">
                        "Combinatorial 3D shape generation via sequential assembly,"</a><br>
                    <i>NeurIPS Workshop on Machine Learning for Engineering Modeling, Simulation, and Design (<strong>ML4Eng-2020</strong>),</i><br>
                    Virtual, December 12, 2020.<br>
                </li>

                <!--lmca-2020-a-->
                <li>
                    Jinhwi Lee*, <strong>Jungtaek Kim</strong>*, Hyunsoo Chung, Jaesik Park, and Minsu Cho (2020),<br>
                    <a href="papers/lmca_2020.pdf">
                        "Fragment relation networks for geometric shape assembly,"</a><br>
                    <i>NeurIPS Workshop on Learning Meets Combinatorial Algorithms (<strong>LMCA-2020</strong>),</i><br>
                    Virtual, December 12, 2020.<br>
                </li>

                <!--neurips-2020-a-->
                <li>
                    Juho Lee*, Yoonho Lee*, <strong>Jungtaek Kim</strong>, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh (2020),<br>
                    <a href="papers/neurips_2020.pdf">
                        "Bootstrapping neural processes,"</a><br>
                    in <i>Advances in Neural Information Processing Systems 33 (<strong>NeurIPS-2020</strong>),</i><br>
                    Virtual, December 6-12, 2020.<br>
<!--                    Vancouver, British Columbia, Canada, December 6-12, 2020.<br>-->
                    Acceptance rate: 1900/9454 = 20.1%<br>
                </li>

                <!--ecml-pkdd-2020-a-->
                <li>
                    <strong>Jungtaek Kim</strong> and Seungjin Choi (2020),<br>
                    <a href="papers/ecmlpkdd_2020.pdf">
                        "On local optimizers of acquisition functions in Bayesian optimization,"</a><br>
                    in <i>Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (<strong>ECML-PKDD-2020</strong>),</i><br>
                    Virtual, September 14-18, 2020.<br>
<!--                    Ghent, Belgium, September 14-18, 2020.<br>-->
                    Acceptance rate: 131/687 = 19.1%<br>
                </li>

                <!--notes-2020-a-->
                <li>
                    <strong>Jungtaek Kim</strong> (2020),<br>
                    <a href="notes/benchmarks_bo.pdf">
                        "Benchmark functions for Bayesian optimization,"</a><br>
                    <i>Notes on Bayesian Optimization,</i><br>
                    July 30, 2020.<br>
                </li>

                <!--automl-2019-a-->
                <li>
                    <strong>Jungtaek Kim</strong>, Michael McCourt, Tackgeun You, Saehoon Kim, and Seungjin Choi (2019),<br>
                    <a href="papers/automl_2019.pdf">
                        "Bayesian optimization over sets,"</a><br>
                    <i>ICML Workshop on Automated Machine Learning (<strong>AutoML-2019</strong>),</i><br>
                    Long Beach, California, USA, June 14, 2019.<br>
                </li>

                <!--icml-2019-a-->
                <li>
                    Juho Lee, Yoonho Lee, <strong>Jungtaek Kim</strong>, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh (2019),<br>
                    <a href="papers/icml_2019.pdf">
                    "Set Transformer: A framework for attention-based permutation-invariant neural networks,"</a><br>
                    in <i>Proceedings of the Thirty-Sixth International Conference on Machine Learning (<strong>ICML-2019</strong>),</i><br>
                    Long Beach, California, USA, June 9-15, 2019.<br>
                    Acceptance rate: 773/3424 = 22.6%<br>
                </li>

                <!--arxiv-1904.05658-->
                <li>
                    <strong>Jungtaek Kim</strong> and Seungjin Choi (2019),<br>
                    <a href="https://arxiv.org/pdf/1905.07540.pdf">
                        "Practical Bayesian optimization with threshold-guided marginal likelihood maximization,"</a><br>
                    <i>arXiv e-prints</i>, arXiv:1905.07540, May 18, 2019.<br>
                </li>

                <!--arxiv-1904.05658-->
                <li>
                    Minseop Park, <strong>Jungtaek Kim</strong>, Saehoon Kim, Yanbin Liu, and Seungjin Choi (2019),<br>
                    <a href="https://arxiv.org/pdf/1904.05658.pdf">
                        "MxML: Mixture of meta-learners for few-shot classification,"</a><br>
                    <i>arXiv e-prints</i>, arXiv:1904.05658, April 11, 2019.<br>
                </li>

                <!--metalearn-2018-a-->
                <li>
                    Minseop Park, Saehoon Kim, <strong>Jungtaek Kim</strong>, Yanbin Liu, and Seungjin Choi (2018),<br>
                    <a href="papers/metalearn_2018.pdf">
                        "TAEML: Task-adaptive ensemble of meta-learners,"</a><br>
                    <i>NeurIPS Workshop on Meta-Learning (<strong>MetaLearn-2018</strong>),</i><br>
                    Montreal, Quebec, Canada, December 8, 2018.<br>
                </li>

                <!--automl-2018-a-->
                <li>
                    <strong>Jungtaek Kim</strong> and Seungjin Choi (2018),<br>
                    <a href="papers/final_automl_2018.pdf">
                        "Automated machine learning for soft voting in an ensemble of tree-based classifiers,"</a><br>
                    <i>ICML Workshop on Automatic Machine Learning (<strong>AutoML-2018</strong>),</i><br>
                    Stockholm, Sweden, July 14, 2018.<br>
                </li>

                <!--icassp-2018-b-->
                <li>
                    Inhyuk Jo, <strong>Jungtaek Kim</strong>, Hyohyeong Kang, Yong-Deok Kim, and Seungjin Choi (2018),<br>
                    <a href="papers/icassp_2018_b.pdf">
                        "Open set recognition by regularizing classifier with fake data generated by generative adversarial networks,"</a><br>
                    in <i>Proceedings of the Forty-Third IEEE International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP-2018</strong>),</i><br>
                    Calgary, Alberta, Canada, April 15-20, 2018.<br>
<!--                    Acceptance rate: 1406/2829 = 49.7%<br>-->
                </li>

                <!--icassp-2018-a-->
                <li>
                    <strong>Jungtaek Kim</strong> and Seungjin Choi (2018),<br>
                    <a href="papers/icassp_2018_a.pdf">
                        "Clustering-guided GP-UCB for Bayesian optimization,"</a><br>
                    in <i>Proceedings of the Forty-Third IEEE International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP-2018</strong>),</i><br>
                    Calgary, Alberta, Canada, April 15-20, 2018.<br>
<!--                    Acceptance rate: 1406/2829 = 49.7%<br>-->
                </li>

                <!--aaai-2018-a-->
                <li>
                    Saehoon Kim, <strong>Jungtaek Kim</strong>, and Seungjin Choi (2018),<br>
                    <a href="papers/aaai_2018.pdf">
                        "On the optimal bit complexity of circulant binary embedding,"</a><br>
                    in <i>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (<strong>AAAI-2018</strong>),</i><br>
                    New Orleans, Louisiana, USA, February 2-7, 2018.<br>
                    Acceptance rate: 933/3800 = 24.6%<br>
                </li>

                <!--bayesopt-2017-a-->
                <li>
                    <strong>Jungtaek Kim</strong>, Saehoon Kim, and Seungjin Choi (2017),<br>
                    <a href="papers/final_bayesopt_2017.pdf">
                        "Learning to transfer initializations for Bayesian hyperparameter optimization,"</a><br>
                    <i>NeurIPS Workshop on Bayesian Optimization (<strong>BayesOpt-2017</strong>),</i><br>
                    Long Beach, California, USA, December 9, 2017.<br>
                </li>

                <!--arxiv-1710.06219-->
                <li>
                    <strong>Jungtaek Kim</strong>, Saehoon Kim, and Seungjin Choi (2017),<br>
                    <a href="https://arxiv.org/pdf/1710.06219.pdf">
                        "Learning to warm-start Bayesian hyperparameter optimization,"</a><br>
                    <i>arXiv e-prints</i>, arXiv:1710.06219, October 17, 2017.<br>
                </li>

                <!--automl-2016-a-->
                <li>
                    <strong>Jungtaek Kim</strong>, Jongheon Jeong, and Seungjin Choi (2016),<br>
                    <a href="papers/final_automl_2016.pdf">
                        "AutoML Challenge: AutoML framework using random space partitioning optimizer,"</a><br>
                    <i>ICML Workshop on Automatic Machine Learning (<strong>AutoML-2016</strong>),</i><br>
                    New York, New York, USA, June 24, 2016.<br>
                    Special track, 3-page<br>
                </li>

            </ol>
        </div>
    </body>
</html>
